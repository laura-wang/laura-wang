<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jiangliu Wang</title>

  <meta name="author" content="Jiangliu Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/curi.jpg">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jiangliu Wang (王江柳)</name>
              </p>
              <p>I am a posdoctoral researcher at <a href="http://ri.cuhk.edu.hk/">CUHK T Stone Robotics Institute</a>, where I work on building machines that understand the real world from videos with minimal supervision.    

              </p>
              <p>
              I obtained my PhD degree from <a href="http://www.cuhk.edu.hk/"> The Chinese University of Hong Kong</a> (CUHK) in 2020, where I was advised by <a href="http://ri.cuhk.edu.hk/yhliu">Prof. Yun-hui Liu</a> and funded by <a href="https://cerg1.ugc.edu.hk/hkpfs/index.html">HKPFS</a>. I obtained my bachelor degree from <a href="https://www.nju.edu.cn/"> Nanjing University</a> in 2015, where I was advised by <a href="https://www.researchgate.net/profile/Wei-Li-262">Prof. Wei Li</a>. During my PhD, I did an internship at <a href="https://ai.tencent.com/ailab/">Tencent AI Lab</a>, where I had a wonderful time working with <a href="https://jianbojiao.com/">Dr. Jianbo Jiao</a>, <a href="http://linchaobao.github.io/">Dr. Linchao Bao</a>, <a href="https://sse.cuhk.edu.cn/en/faculty/liuwei">Dr. Wei Liu</a>, and other labmates.
              </p>
              <p>
              I am seeking collaborations, if you are interested in learning video representations, please feel free to email me.
              </p>


              <p style="text-align:center">
                <a href="data/JiangliuWang-CV.pdf">CV</a> &nbsp/&nbsp
                <!--<a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp-->
                <a href="https://scholar.google.com/citations?user=q6bsitMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <!--<a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp-->
                <a href="https://github.com/laura-wang">Github</a> &nbsp/&nbsp
                <a href="mailto:jiangliuwang@link.cuhk.edu.hk">Email</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/jiangliu_out_circle.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/jiangliu_out_circle.jpg" class="hoverZoomLink"></a> 
              <!JonBarron_circle.jpg>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am interested in learning representation from videos (along with audio as free resources) with minimal supervision. Recently, I also develop an interest in fundamental machine learning theory.
                <!--Representative papers are <span class="highlight">highlighted</span>.-->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!---------Start one research item----------->
          <!------------------------------------------->
          <tr onmouseout="nerfactor_stop()" onmouseover="nerfactor_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfactor_image'>
                  <img src='images/nerfactor_after.png' width="160"></div>
                <img src='images/nerfactor_before.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfactor_start() {
                  document.getElementById('nerfactor_image').style.opacity = "1";
                }
    
                function nerfactor_stop() {
                  document.getElementById('nerfactor_image').style.opacity = "0";
                }
                nerfactor_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/9352025">
              <papertitle>Self-supervised Video Representation Learning by Uncovering Spatio-temporal Statistics</papertitle>
              </a>
              <br>
              <strong> Jiangliu Wang* </strong>,
              <a href="https://jianbojiao.com/">Jianbo Jiao*</a>,
              <a href="http://linchaobao.github.io/">Linchao Bao</a>,
              <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
              <a href="https://sse.cuhk.edu.cn/en/faculty/liuwei">Wei Liu</a>,
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>
              <br>
              <em>T-PAMI</em>, 2021 
              <br>
              <a href="https://ieeexplore.ieee.org/document/9352025">early access</a>
              /
              <a href="https://arxiv.org/pdf/2008.13426.pdf">pdf</a>
              /
              <a href="https://github.com/laura-wang/video_repres_sts">code</a>
              /
              <a href="">bibtex</a>
              <p></p>
              <p>This work is an extension of our <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Self-Supervised_Spatio-Temporal_Representation_Learning_for_Videos_by_Predicting_Motion_and_CVPR_2019_paper.pdf">CVPR 2019 paper</a>. I recommend the ablation study section. Our approach achieved decent performance after just one training epoch.  </p>
            </td>


           <!---------Start one research item----------->
           <!------------------------------------------->

           <tr onmouseout="test_stop()" onmouseover="test_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='test_image'>
                  <img src='images/map_after.png' width="160"></div>
                <img src='images/map_before.png' width="160">
              </div>
              <script type="text/javascript">
                function test_start() {
                  document.getElementById('test_image').style.opacity = "1";
                }
    
                function test_stop() {
                  document.getElementById('test_image').style.opacity = "0";
                }
                test_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Learning_To_Identify_Correct_2D-2D_Line_Correspondences_on_Sphere_CVPR_2021_paper.html">
                <papertitle>Learning To Identify Correct 2D-2D Line Correspondences on Sphere</papertitle>
              </a>
              <br>
              <a href="https://sites.google.com/view/haoangli/homepage">Haoang Li</a>,
              <a href="">Kai Chen</a>, 
              <a href="https://sites.google.com/site/drjizhao/">Ji Zhao</a>,
              <strong>Jiangliu Wang</strong>,
              <a href="http://pyojinkim.com/">Pyojin Kim</a>, 
              <a href="https://scholar.google.com/citations?user=ld6KI1UAAAAJ&hl=en">Zhe Liu</a>, 
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>
              <br>
              <em>CVPR</em>, 2021 
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Learning_To_Identify_Correct_2D-2D_Line_Correspondences_on_Sphere_CVPR_2021_paper.pdf">pdf</a>
              /
              <a href="data/LiCVPR2021.bib">bibtex</a>
              <p></p>
              <p>Line correspondences are mapped into vectors tangent to sphere. Neighboring vectors mapped from inliers exhibit a local trend consistency (analogous to “a school of fish”).</p>
            </td>
          </tr> 
          

           <!---------Start one research item----------->
           <!------------------------------------------->

          <tr onmouseout="nerfbake_stop()" onmouseover="nerfbake_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerfbake_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfbake_15.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/nerfbake_160.png' width="160">
              </div>
              <script type="text/javascript">
                function nerfbake_start() {
                  document.getElementById('nerfbake_image').style.opacity = "1";
                }
    
                function nerfbake_stop() {
                  document.getElementById('nerfbake_image').style.opacity = "0";
                }
                nerfbake_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2008.05861.pdf">
              <papertitle>Self-Supervised Video Representation Learning by Pace Prediction</papertitle>
              </a>
              <br>
              <strong>Jiangliu Wang</strong>,
              <a href="https://jianbojiao.com/">Jianbo Jiao</a>,
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>
              <br>
              <em>ECCV</em>, 2020
              <br>
              <a href="https://arxiv.org/pdf/2008.05861.pdf">pdf</a>
              /
              <a href="https://www.youtube.com/watch?v=5jKry8n5YO8">short video</a>
              /
              <a href="">long video</a>
              /
              <a href="">code</a>
              / 
              <a href="https://nerf.live/#demos">bibtex</a>
              <p></p>
              <p>This work is inspired by the observation that human visual system is sensitive to video pace, e.g., slow motion, a widely used technique in film making.</p>
            </td>
          
          <!---------Start one research item----------->
           <!------------------------------------------->
          <tr onmouseout="ibrnet_stop()" onmouseover="ibrnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='ibrnet_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/ibrnet_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/ibrnet_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function ibrnet_start() {
                  document.getElementById('ibrnet_image').style.opacity = "1";
                }
    
                function ibrnet_stop() {
                  document.getElementById('ibrnet_image').style.opacity = "0";
                }
                ibrnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ibrnet.github.io/">
                <papertitle>Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics</papertitle>
              </a>
              <br>
              <strong> Jiangliu Wang </strong>,
              <a href="https://jianbojiao.com/">Jianbo Jiao</a>,
              <a href="http://linchaobao.github.io/">Linchao Bao</a>,
              <a href="http://www.shengfenghe.com/">Shengfeng He</a>,
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>,
              <a href="https://sse.cuhk.edu.cn/en/faculty/liuwei">Wei Liu</a>
              <br>
              <em>CVPR</em>, 2019
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Self-Supervised_Spatio-Temporal_Representation_Learning_for_Videos_by_Predicting_Motion_and_CVPR_2019_paper.pdf">pdf</a>
              /
              <a href="https://github.com/laura-wang/video_repres_mas">code</a> 
              / 
              <a href="">bibtex</a>
              <p></p>
              <p>Neural networks are asked to predict motion and appearance statistics, including the largest motion area, largest and smallest color diversity areas. </p>
            </td>
          </tr>


           <!---------Start one research item----------->
           <!------------------------------------------->
          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='nerv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/skeleton.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/skeleton.png' width="160">
              </div>
              <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }
    
                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://pratulsrinivasan.github.io/nerv/">
                <papertitle>View-invariant human action recognition based on a 3d bio-constrained skeleton model</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=q-Qqa20AAAAJ&hl=en">Qiang Nie</a>,
              <strong>Jiangliu Wang</strong>,
              <a href="">Xin Wang</a>,
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>
              <br>
              <em>TIP</em>, 2019
              <br>
              <a href="">pdf</a> /
              <a href="">bibtex</a>
              <p></p>
              <p>
              A 3D bio-constrained skeleton model is proposed to recover the corrupted skeletons and encode the body-level motion features into images. 
              </p>
            </td>
          </tr>


                    <!---------Start one research item----------->
           <!------------------------------------------->
         <tr onmouseout="slam_stop()" onmouseover="slam_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='slam_image'>
                  <img src='images/slam_after.png' width="160"></div>
                <img src='images/slam_before.png' width="160">
              </div>
              <script type="text/javascript">
                function slam_start() {
                  document.getElementById('slam_image').style.opacity = "1";
                }
    
                function slam_stop() {
                  document.getElementById('slam_image').style.opacity = "0";
                }
                slam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://yenchenlin.me/inerf/">
                <papertitle>Robot Intelligence for Real World Applications
                </papertitle>
              </a>
              <br>
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>,
              <a href="https://fzheng.me/">Fan Zheng</a>,
              <a href="">Ruibin Guo</a>, 
              <strong>Jiangliu Wang</strong>,
              <a href="https://scholar.google.com/citations?user=q-Qqa20AAAAJ&hl=en">Qiang Nie</a>,
              <a href="">Xin Wang</a>,
              <a href="https://www.wangzerui.com/">Zerui Wang</a>,
              <br>
              <em>Chinese Journal of Electronics</em>, 2018 
              <br>
              <a href="http://yenchenlin.me/inerf/">pdf</a> /
              <a href="https://www.youtube.com/watch?v=eQuCZaQN0tI">bibtex</a>
              <p></p>
              <p>This is an editor invitation paper. A brief review is presented to introduce our recent works on machine intelligence for real-world applications of robots. One technology <br> leads to a startup company <a href="https://www.visionnav.com/">VisionNav</a>.
              </p>
            </td>
          </tr> 


           <!---------Start one research item----------->
           <!------------------------------------------->
          <tr onmouseout="winr_stop()" onmouseover="winr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/linear_velocity.png" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/linear_velocity.png' width="160">
              </div>
              <script type="text/javascript">
                function winr_start() {
                  document.getElementById('winr_image').style.opacity = "1";
                }
                function winr_stop() {
                  document.getElementById('winr_image').style.opacity = "0";
                }
                winr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://www.matthewtancik.com/learnit">
                <papertitle>Kinematics features for 3D action recognition using two-stream CNN</papertitle>
              </a>
              <br>
              
              <strong> Jiangliu Wang </strong>, 
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>
              <br>
              <!--<em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>-->
              <em>WCICA</em>, 2018
              <br>
              <a href="http://www.matthewtancik.com/learnit">pdf</a> /
              <a href="https://arxiv.org/abs/2012.02189">bibtex</a> 
              <p></p>
              <p>Temporal encoded kinematics features are proposed for action recognition, which compute the linear velocity and orientation displacement based on human skeleton data.</p>
            </td>
          </tr>
          

          <!---------Start one research item----------->
           <!------------------------------------------->
         <tr onmouseout="child_stop()" onmouseover="child_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='child_image'>
                  <img src='images/child.png' width="160"></div>
                <img src='images/child.png' width="160">
              </div>
              <script type="text/javascript">
                function child_start() {
                  document.getElementById('child_image').style.opacity = "1";
                }
    
                function child_stop() {
                  document.getElementById('child_image').style.opacity = "0";
                }
                slam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://nerf-w.github.io/">
                <papertitle>A child caring robot for the dangerous behavior detection based on the object recognition and human action recognitions</papertitle>
              </a>
              <br>
               <a href="https://scholar.google.com/citations?user=q-Qqa20AAAAJ&hl=en">Qiang Nie</a>,
              <a href="">Xin Wang</a>,
              <strong>Jiangliu Wang</strong>,
              <a href="http://ri.cuhk.edu.hk/yhliu">Yun-hui Liu</a>
              <br>
              <em>ROBIO</em>, 2018
              <br>
              <a href="https://arxiv.org/abs/2008.02268">pdf</a> /
              <a href="https://www.youtube.com/watch?v=mRAKVQj5LRA">bibtex</a>
              <p></p>
              <p> A caring robot is developed to detect dangerous behavior of children in the domestic environment based on action recognition and object recognition technologies.</p>
            </td>
          </tr> 
          
          <!----------------------Before PhD-------------------->

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Before PhD</heading>
              <p>During my undergraduate study, I was lucky enough to work with <a href="https://www.researchgate.net/profile/Wei-Li-262">Prof. Wei Li</a> on a defense–intrusion interaction optimization problem. This work is published in a Tier 1 applied mathematics journal.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!---------Start one research item----------->
           <!------------------------------------------->

          <tr onmouseout="phase_stop()" onmouseover="phase_start()" >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='phase_image'>
                  <img src='images/phase2.jpg' width="160"></div>
                <img src='images/phase1.jpg' width="160">
              </div>
              <script type="text/javascript">
                function phase_start() {
                  document.getElementById('phase_image').style.opacity = "1";
                }
    
                function phase_stop() {
                  document.getElementById('phase_image').style.opacity = "0";
                }
                slam_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://markboss.me/publication/2021-nerd/">
                <papertitle>Motion patterns and phase-transition of a defender–intruder problem and optimal interception strategy of the defender</papertitle>
              </a>
              <br>
              <strong>Jiangliu Wang</strong>,
              <a href="">Wei Li</a>
              <br>
              <em>Communications in Nonlinear Science and Numerical Simulation</em>, 2015
              <br>
              <a href="https://markboss.me/publication/2021-nerd/">pdf</a> /
              <a href="https://www.youtube.com/watch?v=JL-qMTXw9VU">bibtex</a>
              <p></p>
              <p>
              An optimal interception strategy of the defender is provided with interpretations of its physical meaning, which depends on relative mobility of the intruder and defender.
              </p>
            </td>
          </tr>

       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Thesis</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src='images/cuhk.png' width="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="data/Thesis-Jiangliu-Wang.pdf">
              <papertitle>Self-Supervised Video Representation Learning</papertitle>
              </a>
              <br>
              <strong>Jiangliu Wang</strong>
              <br>
              September, 2020 
              <br>
              <a href="data/Thesis-Jiangliu-Wang.pdf">pdf</a> &nbsp/&nbsp
              <a href="data/Presentation-Jiangliu-Wang.pptx">slides</a>
              <p></p>
              
            </td>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading>
              <p>
              Reviewer of 
              <a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>, 
              <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>, 
              <a href=" https://icml21ssl.github.io/">ICML 2021 Workshop on SSL</a>,
              <a href="http://sslneuips20.github.io">NeurIPS 2020 Workshop on SSL</a>,
              <a href="https://ewh.ieee.org/soc/ras/conf/fullysponsored/icra/ICRA2020/www.icra2020.org/index.html">ICRA 2020</a>,
              <a href="https://www.iros2019.org/">IROS 2019</a>,
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=5962385">T-NNLS</a>,
              <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=100">RAM</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Page template from <a href="https://github.com/jonbarron/jonbarron_website">Jonathan T. Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
